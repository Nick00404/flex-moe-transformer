# flex-moe-transformer
"A flexible Mixture of Experts (MoE) Transformer implementation in PyTorch featuring dynamic routing, expert balancing, and seamless integration with standard architectures. Supports advanced routing strategies, capacity scaling, and comprehensive monitoring."
